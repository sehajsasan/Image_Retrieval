{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20bb428-bde9-471f-9fac-9b7fab37d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Feature import feature\n",
    "from Utils import recall, recall2, recall2_batch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from Reader import ImageReader\n",
    "from Utils import tra_transforms, eva_transforms\n",
    "from color_lib import RGBmean, RGBstdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10e2ad9-f949-41bb-bbb9-970c232bcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = \"HAR\"\n",
    "imgsize = 160\n",
    "tra_transform = tra_transforms(imgsize, RGBmean[Data], RGBstdv[Data])\n",
    "eva_transform = eva_transforms(imgsize, RGBmean[Data], RGBstdv[Data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef27596f-f610-4014-a742-30d3bc2e7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca2032baae84b68bcf142784d7e6d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting model: resnet50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets_dict = torch.load(\"/notebooks/Image_Retrieval/EasyPositiveHardNegative-master/data_dict_emb_test.pth\")\n",
    "# dsets_dict = {p: ImageReader(data_dict[p], eva_transform) for p in phase}\n",
    "model = models.resnet50(pretrained=True)\n",
    "print('Setting model: resnet50')\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 64)\n",
    "checkpoint = torch.load(\"/notebooks/Image_Retrieval/EasyPositiveHardNegative-master/_result/EPSHN/HAR_R50/G16_lr0.03/model_state_dict_R5050.pth\")\n",
    "model.load_state_dict(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82c4656-df3b-4a76-a078-a84979c4782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16e663ef-50d8-4627-bbe6-610f6b8adf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = ImageReader(dsets_dict['gallery'], tra_transform)\n",
    "dsets1 = ImageReader(dsets_dict['query'], tra_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce5b7a8d-650b-448e-ab58-e0045ed5428b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'calling',\n",
       " 1: 'clapping',\n",
       " 2: 'cycling',\n",
       " 3: 'dancing',\n",
       " 4: 'drinking',\n",
       " 5: 'eating',\n",
       " 6: 'fighting',\n",
       " 7: 'hugging',\n",
       " 8: 'laughing',\n",
       " 9: 'listening_to_music',\n",
       " 10: 'running',\n",
       " 11: 'sitting',\n",
       " 12: 'sleeping',\n",
       " 13: 'texting',\n",
       " 14: 'using_laptop'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = dsets1.class_to_idx\n",
    "(class_names)\n",
    "inv_map = {v: k for k, v in class_names.items()}\n",
    "inv_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ed78256f-e61d-4592-bc14-20b00d2ec9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_class_name_from_id(dsets1.imgs[55][1],inv_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b73c2107-9ae9-403d-8318-40ae5d8a0bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calling'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_class_name_from_id(class_id_of_image,class_name_dict):\n",
    "    class_name = class_name_dict[class_id_of_image]\n",
    "    return class_name\n",
    "class_name = get_class_name_from_id(0,inv_map)\n",
    "class_name    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d9d6818-1655-4cd3-9474-7c7f558eaa15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "acc = recall2(feature(dsets, model),\n",
    "                      feature(dsets1, model), \n",
    "                      dsets.idx_to_class, \n",
    "                      dsets1.idx_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e9ccd5d-32d9-4ca2-ae5c-2ec611615560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "Fvec_val = feature(dsets1, model)\n",
    "Fvec_gal = feature(dsets, model)\n",
    "imgLab_val = dsets1.idx_to_class\n",
    "imgLab_gal = dsets.idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e731992-b366-420d-b37f-3263cf1f1315",
   "metadata": {},
   "source": [
    "## mAP@k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8bfc8d27-b553-45ec-b4b7-73dc334b4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(imgLab_val)\n",
    "imgLab_val = torch.LongTensor([imgLab_val[i] for i in range(len(imgLab_val))])\n",
    "imgLab_gal = torch.LongTensor([imgLab_gal[i] for i in range(len(imgLab_gal))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "aa08127d-f040-4c22-952a-130dfd8706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Fvec_val.mm(torch.t(Fvec_gal))\n",
    "preds_1 = []\n",
    "_,idx = D.max(1)\n",
    "imgPre = imgLab_gal[idx]\n",
    "preds_1.append(imgPre)\n",
    "A = (imgPre==imgLab_val).float()\n",
    "recall_1 = (torch.sum(A)/N).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "e8764afe-a1d5-407e-a350-df71000e91c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753333330154419"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "50d8658d-e237-4649-b88e-a5f147da8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_1=[]\n",
    "rank_1 = [0]*150\n",
    "k=1\n",
    "for i in range(150):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0\n",
    "    for j in range(k):\n",
    "        if((preds_1[j][i])==imgLab_val[i]):\n",
    "            # print(preds1[i])\n",
    "            if(rank_1[i]==0):\n",
    "                rank_1[i] = (j+1)\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / (j + 1)\n",
    "            # print(precision_sum)\n",
    "    ap = precision_sum / k\n",
    "        # print(ap)\n",
    "            \n",
    "    precision_1.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5d840002-e6ed-4ee9-adb1-660429c6edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_precision_1 = sum(precision_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "72c8641c-a7b3-441f-9511-b128a49a7767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533333333333333"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_1 = sum_precision_1/N\n",
    "map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b8ad154c-7fa0-4f7a-9f99-5f963a4d0246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7533333333333333"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rank_1)/150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "675e0715-1539-490b-a7fe-6fd3c5398ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 5,\n",
       " 8,\n",
       " 9,\n",
       " 13,\n",
       " 16,\n",
       " 35,\n",
       " 44,\n",
       " 45,\n",
       " 47,\n",
       " 50,\n",
       " 51,\n",
       " 55,\n",
       " 60,\n",
       " 64,\n",
       " 66,\n",
       " 70,\n",
       " 74,\n",
       " 75,\n",
       " 78,\n",
       " 79,\n",
       " 92,\n",
       " 93,\n",
       " 97,\n",
       " 98,\n",
       " 100,\n",
       " 110,\n",
       " 111,\n",
       " 117,\n",
       " 118,\n",
       " 125,\n",
       " 132,\n",
       " 135,\n",
       " 138,\n",
       " 139,\n",
       " 145,\n",
       " 148]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_relevant_img_1=[]\n",
    "for i in range(len(rank_1)):\n",
    "    if rank_1[i] == 0:\n",
    "        no_relevant_img_1.append(i)\n",
    "(no_relevant_img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c706ab99-b851-4913-961c-822fb4e5dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "query_images = 150\n",
    "columns = 3\n",
    "rows = 2  # 1 query image + 50 retrieved images\n",
    "\n",
    "# Adjust the size of your images as necessary\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "\n",
    "# Set up canvas size\n",
    "canvas_width = 400\n",
    "canvas_height =  400\n",
    "\n",
    "for i in range(query_images):\n",
    "    # Create a blank canvas\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Load and paste the query image\n",
    "    query_image = dsets1.imgs[i][0]\n",
    "    query_img_class = get_class_name_from_id(dsets1.imgs[i][1], inv_map)\n",
    "    image = Image.open(query_image)\n",
    "    image = image.resize((image_width, image_height))\n",
    "    canvas.paste(image, (0, 0))\n",
    "    draw.text((10, 10), f\"Query:{query_img_class}\", fill=\"white\")\n",
    "\n",
    "    # Load and paste the retrieved images\n",
    "    candidates_idxx = []\n",
    "    candidates_idxx.append(idx[i].tolist())\n",
    "    \n",
    "    for j, idxx in enumerate(candidates_idxx):\n",
    "        image_path = dsets.imgs[idxx][0]\n",
    "        image_class = get_class_name_from_id(dsets.imgs[idxx][1], inv_map)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((image_width, image_height))\n",
    "        x_pos = (j % columns+2) * image_width\n",
    "        y_pos = ((j // columns) + 1) * image_height  # +1 to shift down past the query image\n",
    "        canvas.paste(image, (x_pos, y_pos))\n",
    "        draw.text((x_pos + 10, y_pos + 10), f\"Retrieved:{image_class}\", fill=\"white\")\n",
    "\n",
    "    # Save the canvas as an image\n",
    "    canvas.save(f'results_k_1/query_{i}_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b8d3723-e2f2-4138-bc62-f7c9f80ce4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0236, -0.1163, -0.2755,  ...,  0.4113,  0.5511,  0.2319],\n",
       "        [ 0.0830,  0.3949,  0.0047,  ...,  0.1088,  0.0916,  0.1578],\n",
       "        [ 0.3001, -0.0565,  0.2835,  ..., -0.1077, -0.1218,  0.1212],\n",
       "        ...,\n",
       "        [ 0.2729,  0.4691,  0.2628,  ...,  0.6230,  0.3767,  0.6189],\n",
       "        [ 0.1831,  0.1627,  0.4251,  ...,  0.3030,  0.1257,  0.2952],\n",
       "        [ 0.1420,  0.3169,  0.1142,  ...,  0.5709,  0.4008,  0.4919]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3ed2a7a7-b8da-40be-8218-acb3c070a8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150, 1000])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681fa6f4-8e9e-4bb1-bb02-e158dcacb903",
   "metadata": {},
   "source": [
    "### mAP @ K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "61618dbc-26d3-411e-8f4d-3640d401a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "929d23d4-bb1c-4422-9993-86a47b3d6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(imgLab_val)\n",
    "imgLab_val = torch.LongTensor([imgLab_val[i] for i in range(len(imgLab_val))])\n",
    "imgLab_gal = torch.LongTensor([imgLab_gal[i] for i in range(len(imgLab_gal))])\n",
    "\n",
    "D = Fvec_val.mm(torch.t(Fvec_gal))\n",
    "_,idx = D.topk(rank[-1])\n",
    "acc_list = []\n",
    "preds_10 = []\n",
    "for r in rank:\n",
    "    A = 0\n",
    "    for i in range(r):\n",
    "        imgPre = imgLab_gal[idx[:,i]]\n",
    "        preds_10.append(imgPre)\n",
    "        A += (imgPre==imgLab_val).float()\n",
    "    acc_list.append((torch.sum((A>0).float())/N).item())\n",
    "recall_10=torch.Tensor(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "65c2831f-20c8-42ab-b367-74ace82c52fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8933])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3b6519c-906c-406c-a188-5915d1fca493",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_10=[]\n",
    "rank_10 = [0]*150\n",
    "k=10\n",
    "for i in range(N):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0\n",
    "    for j in range(k):\n",
    "        if((preds_10[j][i])==imgLab_val[i]):\n",
    "            # print(preds1[i])\n",
    "            if(rank_10[i]==0):\n",
    "                rank_10[i] = (j+1)\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / (j + 1)\n",
    "            # print(precision_sum)\n",
    "    ap = precision_sum / k\n",
    "        # print(ap)\n",
    "            \n",
    "    precision_10.append(ap)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "99b4766b-e10a-4719-8c3b-41c8ed72ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_precision_10 = sum(precision_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "9f37a1f7-7def-4f5c-b008-79b169a26b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7059148148148149"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_10 = sum_precision_10/N\n",
    "map_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c34b6e05-d07e-42e5-b1b8-e6011f345656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.22"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rank_10)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0659942-35ce-415d-a932-5f74cb65b735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 9, 16, 45, 50, 55, 70, 74, 75, 92, 93, 97, 125, 132, 139, 148]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_relevant_img_10=[]\n",
    "for i in range(len(rank_10)):\n",
    "    if rank_10[i] == 0:\n",
    "        no_relevant_img_10.append(i)\n",
    "no_relevant_img_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ce35a-1896-4a03-a255-6ca59f3fda21",
   "metadata": {},
   "source": [
    "Visualise or save retrieval results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "319abde0-4888-4745-905d-0709d4477dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "query_images = 150\n",
    "columns = 3\n",
    "rows = 11  # 1 query image + 50 retrieved images\n",
    "\n",
    "# Adjust the size of your images as necessary\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "\n",
    "# Set up canvas size\n",
    "canvas_width = 700\n",
    "canvas_height =  700\n",
    "\n",
    "for i in range(query_images):\n",
    "    # Create a blank canvas\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Load and paste the query image\n",
    "    query_image = dsets1.imgs[i][0]\n",
    "    query_img_class = get_class_name_from_id(dsets1.imgs[i][1], inv_map)\n",
    "    image = Image.open(query_image)\n",
    "    image = image.resize((image_width, image_height))\n",
    "    canvas.paste(image, (0, 0))\n",
    "    draw.text((10, 10), f\"Query:{query_img_class}\", fill=\"white\")\n",
    "\n",
    "    # Load and paste the retrieved images\n",
    "    candidates_idxx = idx[i].tolist()\n",
    "    for j, idxx in enumerate(candidates_idxx):\n",
    "        image_path = dsets.imgs[idxx][0]\n",
    "        image_class = get_class_name_from_id(dsets.imgs[idxx][1], inv_map)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((image_width, image_height))\n",
    "        x_pos = (j % columns+2) * image_width\n",
    "        y_pos = ((j // columns) + 1) * image_height  # +1 to shift down past the query image\n",
    "        canvas.paste(image, (x_pos, y_pos))\n",
    "        draw.text((x_pos + 10, y_pos + 10), f\"Retrieved:{image_class}\", fill=\"white\")\n",
    "\n",
    "    # Save the canvas as an image\n",
    "    canvas.save(f'results_k_10/query_{i}_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639352cd-077e-4278-b4a7-229fb224c516",
   "metadata": {},
   "source": [
    "## mAP@k=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "01294b23-743e-43b4-9403-a86138375cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "56717072-1a2d-4f31-91e9-006855830006",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(imgLab_val)\n",
    "imgLab_val = torch.LongTensor([imgLab_val[i] for i in range(len(imgLab_val))])\n",
    "imgLab_gal = torch.LongTensor([imgLab_gal[i] for i in range(len(imgLab_gal))])\n",
    "\n",
    "D = Fvec_val.mm(torch.t(Fvec_gal))\n",
    "_,idx = D.topk(rank[-1])\n",
    "acc_list = []\n",
    "preds_50 = []\n",
    "for r in rank:\n",
    "    A = 0\n",
    "    for i in range(r):\n",
    "        imgPre = imgLab_gal[idx[:,i]]\n",
    "        preds_50.append(imgPre)\n",
    "        A += (imgPre==imgLab_val).float()\n",
    "    acc_list.append((torch.sum((A>0).float())/N).item())\n",
    "recall_50=torch.Tensor(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "925a243c-a169-4b3f-afaa-28fabffd8c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9667])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e23de18c-8bd4-4bab-8cdc-f0f681a38bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_50=[]\n",
    "rank_50 = [0]*150\n",
    "k=50\n",
    "for i in range(N):\n",
    "    relevant_count = 0\n",
    "    precision_sum = 0\n",
    "    for j in range(k):\n",
    "        if((preds_50[j][i])==imgLab_val[i]):\n",
    "            # print(preds1[i])\n",
    "            if(rank_50[i]==0):\n",
    "                rank_50[i] = (j+1)\n",
    "            relevant_count += 1\n",
    "            precision_sum += relevant_count / (j + 1)\n",
    "            # print(precision_sum)\n",
    "    ap = precision_sum / k\n",
    "        # print(ap)\n",
    "            \n",
    "    precision_50.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "282250db-da2c-4f88-83e4-186d7668c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_precision_50 = sum(precision_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0991eb0d-6eae-4873-b07b-0dfb5127b094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6097017321715196"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_50 = sum_precision_50/N\n",
    "map_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8600bd68-7b7e-41da-a7b7-4ba09ba67ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8733333333333335"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rank_50)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ca1c98d5-39e3-49e1-a151-f924ce3dec34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 45, 55, 75, 93]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_relevant_img_50=[]\n",
    "for i in range(len(rank_50)):\n",
    "    if rank_50[i] == 0:\n",
    "        no_relevant_img_50.append(i)\n",
    "no_relevant_img_50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5371f57c-8df4-41c4-862b-49c84db271a1",
   "metadata": {},
   "source": [
    "Visualise results for k=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "596a2215-5650-46e4-ba57-c0377eda7be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "query_images = 150\n",
    "columns = 3\n",
    "rows = 51  # 1 query image + 50 retrieved images\n",
    "\n",
    "# Adjust the size of your images as necessary\n",
    "image_width = 128\n",
    "image_height = 128\n",
    "\n",
    "# Set up canvas size\n",
    "canvas_width = 700\n",
    "canvas_height =  1000\n",
    "\n",
    "for i in range(2):\n",
    "    # Create a blank canvas\n",
    "    canvas = Image.new('RGB', (canvas_width, canvas_height), (255, 255, 255))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "\n",
    "    # Load and paste the query image\n",
    "    query_image = dsets1.imgs[i][0]\n",
    "    query_img_class = get_class_name_from_id(dsets1.imgs[i][1], inv_map)\n",
    "    image = Image.open(query_image)\n",
    "    image = image.resize((image_width, image_height))\n",
    "    canvas.paste(image, (0, 0))\n",
    "    draw.text((10, 10), f\"Query:{query_img_class}\", fill=\"white\")\n",
    "\n",
    "    # Load and paste the retrieved images\n",
    "    candidates_idxx = idx[i].tolist()\n",
    "    for j, idxx in enumerate(candidates_idxx):\n",
    "        image_path = dsets.imgs[idxx][0]\n",
    "        image_class = get_class_name_from_id(dsets.imgs[idxx][1], inv_map)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.resize((image_width, image_height))\n",
    "        x_pos = (j % columns+2) * image_width\n",
    "        y_pos = ((j // columns) + 1) * image_height  # +1 to shift down past the query image\n",
    "        canvas.paste(image, (x_pos, y_pos))\n",
    "        draw.text((x_pos + 10, y_pos + 10), f\"Retrieved:{image_class}\", fill=\"white\")\n",
    "\n",
    "    # Save the canvas as an image\n",
    "    canvas.save(f'results_k_50/query_{i}_results.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a8fd3-1ba3-4d3b-b5f1-459103a9e781",
   "metadata": {},
   "source": [
    "## Get mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1166ccc1-6c9f-4592-bb6e-ab9a7d6434cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20df55b-757c-4337-be8a-f86018740437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sleeping',\n",
       " 'clapping',\n",
       " 'laughing',\n",
       " 'listening_to_music',\n",
       " 'dancing',\n",
       " 'calling',\n",
       " 'sitting',\n",
       " 'hugging',\n",
       " 'using_laptop',\n",
       " 'running',\n",
       " 'eating',\n",
       " 'texting',\n",
       " 'cycling',\n",
       " 'fighting',\n",
       " 'drinking']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"/notebooks/Image_Retrieval/Hierarchy-image-retrieval-main/dataset/train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ccb2160-0ab4-4d34-8787-b509863a936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_path = \"/notebooks/Image_Retrieval/Hierarchy-image-retrieval-main/dataset/train_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "852bf6d5-0164-4b5a-a30e-0ea491b7d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.Resize((160,160)),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c861bcda-00a6-4e3f-a399-c8220bd53a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder(root=training_dataset_path,transform=training_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb1394a-aba5-4bbc-9f8b-3930dae2440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=128,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c3e9d8-784c-4ea1-a7c8-b2350a12d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(loader):\n",
    "    mean = 0\n",
    "    std = 0\n",
    "    total_images_count = 0\n",
    "    for images,_ in loader:\n",
    "        images_count_in_a_batch = images.size(0)\n",
    "        images = images.view(images_count_in_a_batch,images.size(1),-1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += images_count_in_a_batch\n",
    "    \n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean,std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a976c51-1a8b-46a4-af32-603584c8b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5717, 0.5368, 0.5057]), tensor([0.2396, 0.2372, 0.2406]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mean_and_std(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ced34f-0096-48a9-82d2-4b5d89e0551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
